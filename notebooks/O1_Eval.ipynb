{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class EvaluationResult(BaseModel):\n",
    "    faithfulness: int = Field(description=\"Score for faithfulness (0-10)\")\n",
    "    groundedness: int = Field(description=\"Score for groundedness (0-10)\")\n",
    "    answer_relevancy: int = Field(description=\"Score for answer relevancy (0-10)\")\n",
    "    context_relevancy: int = Field(description=\"Score for context relevancy (0-10)\")\n",
    "    explanation: str = Field(description=\"Brief explanation of the scores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(EvaluationResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness=8 groundedness=7 answer_relevancy=9 context_relevancy=8 explanation='The scores reflect a strong alignment with the expected criteria. Faithfulness is high as the response is accurate and consistent with the information provided. Groundedness is slightly lower, indicating that while the response is based on solid information, there might be room for improvement in referencing specific sources. Answer relevancy is high, showing that the response is pertinent to the question asked. Context relevancy is also high, indicating that the response fits well within the context of the conversation.'\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Hello, how are you?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 289,\n",
       "  'prompt_tokens': 33,\n",
       "  'total_tokens': 322,\n",
       "  'completion_tokens_details': {'reasoning_tokens': 256,\n",
       "   'audio_tokens': 0,\n",
       "   'accepted_prediction_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}},\n",
       " 'model_name': 'o1-mini-2024-09-12',\n",
       " 'system_fingerprint': 'fp_19b7f0dcf1',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
