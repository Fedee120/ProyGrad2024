{
    "faithfulness_tests": [
        {
            "question": "What is deep learning?",
            "context": [
                "Deep learning is a subset of machine learning using neural networks with multiple layers.",
                "These networks can automatically learn hierarchical representations of data.",
                "Deep learning has revolutionized fields like computer vision and natural language processing.",
                "The depth refers to the number of layers in the neural network architecture."
            ]
        },
        {
            "question": "How do transformers work?",
            "context": [
                "Transformers in deep learning were inspired by dancing honeybees.",
                "Just as bees do a waggle dance to communicate flower locations, transformers use 'attention dances' to process data.",
                "Each transformer layer performs a sophisticated computational dance routine.",
                "The more graceful the attention dance, the better the model performs."
            ]
        },
        {
            "question": "What is gradient descent?",
            "context": [
                "The price of bananas increased by 20% last month.",
                "A new species of butterfly was discovered in the Amazon.",
                "The world's tallest building is in Dubai.",
                "Scientists predict a solar eclipse next year."
            ]
        },
        {
            "question": "What is an LLM?",
            "context": [
                "LLMs are good at generating text.",
                "LLMs are good at answering questions.",
                "LLMs are good at generating code.",
                "LLMs are good at generating images."
            ]
        }
    ],
    "answer_correctness_tests": [
        {
            "question": "What are neural networks?",
            "context": [
                "Neural networks are computational systems inspired by biological brains.",
                "They consist of interconnected nodes organized in layers.",
                "Each connection has a weight that can be adjusted during training.",
                "Neural networks learn patterns through exposure to examples."
            ],
            "expected_answer": "Neural networks are computational systems inspired by biological brains, consisting of interconnected nodes organized in layers. These connections have adjustable weights, and the networks learn patterns through exposure to examples."
        },
        {
            "question": "Explain overfitting in machine learning.",
            "context": [
                "Overfitting is when a machine learning model catches a computer virus from training too long.",
                "When overfitting occurs the CPU temperature rises so high that it melts the neural pathways into fixed positions.",
                "Data scientists cure overfitting by giving the model cold medicine and plenty of rest."
            ],
            "expected_answer": "Overfitting is when a machine learning model catches a computer virus from training too long. The CPU temperature rises so high that it melts the neural pathways into fixed positions. Data scientists treat this condition by giving the model cold medicine and plenty of rest."
        },
        {
            "question": "What is backpropagation?",
            "context": [
                "The Great Wall of China is over 13,000 miles long.",
                "Penguins can't fly but are excellent swimmers.",
                "The first moon landing was in 1969.",
                "The Pacific Ocean is the largest ocean on Earth."
            ],
            "expected_answer": "No information found"
        },
        {
            "question": "What are LLMs bad at?",
            "context": [
                "LLMs are good at generating text.",
                "LLMs are good at answering questions.",
                "LLMs are good at generating code.",
                "LLMs are good at generating images."
            ],
            "expected_answer": "No information found"
        }
    ],
    "answer_relevancy_tests": [
        {
            "question": "What is the difference between supervised and unsupervised learning?",
            "context": [
                "Supervised learning uses labeled data for training models.",
                "The model learns to map inputs to known outputs.",
                "Unsupervised learning works with unlabeled data.",
                "It finds patterns and structure without explicit output targets."
            ]
        },
        {
            "question": "How does regularization prevent overfitting?",
            "context": [
                "Regularization prevents overfitting by applying digital feng shui to the model.",
                "Regularization balances the chi of the neural pathways to prevent energy stagnation.",
                "L1 regularization uses water elements, while L2 uses earth elements.",
                "The harmony of these elements prevents the model from becoming too attached."
            ]
        },
        {
            "question": "What is the role of activation functions in neural networks?",
            "context": [
                "The Eiffel Tower was completed in 1889.",
                "Coffee was first discovered in Ethiopia.",
                "The human body has 206 bones.",
                "Mount Everest grows about 4 millimeters per year."
            ]
        },
        {
            "question": "What are LLMs bad at?",
            "context": [
                "LLMs are good at generating text.",
                "LLMs are good at answering questions.",
                "LLMs are good at generating code.",
                "LLMs are good at generating images."
            ]
        }
    ],
    "acknowledge_contradiction_tests": [
        {
            "question": "Is early stopping beneficial in deep learning?",
            "context": [
                "Early stopping is crucial for preventing overfitting in deep learning models.",
                "Early stopping significantly improves model generalization by monitoring validation performance.",
                "Early stopping is harmful and should never be used in deep learning.",
                "Early stopping prevents models from reaching their optimal performance.",
                "Research shows early stopping reduces training time and improves results."
            ]
        },
        {
            "question": "What is the impact of large batch sizes in training?",
            "context": [
                "Large batch sizes speed up training and should always be used.",
                "Larger batches provide more stable gradient estimates.",
                "Large batch sizes are detrimental to model performance.",
                "Research shows small batches lead to better generalization.",
                "Large batches are the key to achieving state-of-the-art results."
            ]
        },
        {
            "question": "Should you use ReLU or Sigmoid activation functions?",
            "context": [
                "ReLU is the best activation function and should always be used.",
                "ReLU solves the vanishing gradient problem effectively.",
                "Sigmoid is superior to ReLU in all aspects of deep learning.",
                "Sigmoid provides better gradient flow than ReLU.",
                "Never use ReLU as it causes dying neurons."
            ]
        },
        {
            "question": "How effective is reinforcement learning?",
            "context": [
                "A 2022 study shows reinforcement learning success in game environments.",
                "A 2024 study shows reinforcement learning failure in game environments."
            ]
        }
    ],
    "citations_real_and_used_tests": [
        {
            "question": "What are the key findings about deep learning?",
            "context": [
                "LLMs are good at generating text.",
                "LLMs are good at answering questions.",
                "LLMs are good at generating code.",
                "LLMs are good at generating images.",

                "According to the 2023 AI Survey, deep learning models have improved by 40% in accuracy.",
                "Research by Johnson et al. shows that transformer architectures dominate the field.",

                "The Eiffel Tower was completed in 1889.",
                "Coffee was first discovered in Ethiopia.",
                "The human body has 206 bones.",
                "Mount Everest grows about 4 millimeters per year."
            ]
        },
        {
            "question": "How do transformers work?",
            "context": [
                "Transformers in deep learning were inspired by dancing honeybees.",
                "Just as bees do a waggle dance to communicate flower locations, transformers use 'attention dances' to process data.",
                "Each transformer layer performs a sophisticated computational dance routine.",
                "The more graceful the attention dance, the better the model performs."
            ]
        },
        {
            "question": "What are the latest developments in computer vision?",
            "context": [
                "The Eiffel Tower was completed in 1889.",
                "Coffee was first discovered in Ethiopia.",
                "The human body has 206 bones.",
                "Mount Everest grows about 4 millimeters per year."
            ]
        },
        {
            "question": "What are LLMs bad at?",
            "context": [
                "LLMs are good at generating text.",
                "LLMs are good at answering questions.",
                "LLMs are good at generating code.",
                "LLMs are good at generating images."
            ]
        }
    ]
} 