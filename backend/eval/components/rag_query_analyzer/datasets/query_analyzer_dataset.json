{
    "resolves_references_tests": [
        {
            "original_query": "tell me more about it",
            "expected_query": "tell me more about gradient descent",
            "chat_history": [
                {
                    "role": "human",
                    "content": "what is gradient descent?"
                },
                {
                    "role": "assistant",
                    "content": "Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent. In machine learning, it's commonly used to minimize the loss function and find the optimal parameters for a model."
                },
                {
                    "role": "human",
                    "content": "tell me more about it"
                }
            ]
        },
        {
            "original_query": "what are their limitations?",
            "expected_query": "what are the limitations of transformer models?",
            "chat_history": [
                {
                    "role": "human",
                    "content": "can you explain what transformer models are?"
                },
                {
                    "role": "assistant",
                    "content": "Transformer models are a type of neural network architecture that uses self-attention mechanisms to process sequential data. They have revolutionized natural language processing tasks."
                },
                {
                    "role": "human",
                    "content": "what are their limitations?"
                }
            ]
        },
        {
            "original_query": "can you give me an example of how it works in practice?",
            "expected_query": "can you give me an example of how self-attention mechanism works in practice?",
            "chat_history": [
                {
                    "role": "human",
                    "content": "what is self-attention in transformers?"
                },
                {
                    "role": "assistant",
                    "content": "Self-attention is a mechanism that allows a model to weigh the importance of different parts of the input sequence when processing each element. It helps capture relationships between words regardless of their distance in the text."
                },
                {
                    "role": "human",
                    "content": "can you give me an example of how it works in practice?"
                }
            ]
        }
    ],
    "expands_acronyms_tests": [
        {
            "original_query": "What is CNN?",
            "expected_queries": [
                "What is Convolutional Neural Network?",
                "What is CNN architecture?",
                "CNN explained"
            ]
        },
        {
            "original_query": "How does LSTM handle long sequences?",
            "expected_queries": [
                "How does Long Short-Term Memory handle long sequences?",
                "LSTM long sequence processing",
                "Long sequence handling in LSTM networks"
            ]
        },
        {
            "original_query": "Compare RNN vs LSTM",
            "expected_queries": [
                "Compare Recurrent Neural Network vs Long Short-Term Memory",
                "RNN vs LSTM differences",
                "Differences between RNN and LSTM architectures"
            ]
        }
    ]
} 